{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-479d219221ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Generated list of sentences..\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "print('Processing text dataset')\n",
    "\n",
    "tree = ET.parse(\"./Restaurants.xml\")\n",
    "# tree = ET.parse(\"./Laptop.xml\")\n",
    "corpus = tree.getroot()\n",
    "sentences = [] # List of list of sentences.\n",
    "sent = corpus.findall('.//sentence')\n",
    "for s in sent:\n",
    "    sentences.append(s.find('text').text)\n",
    "\n",
    "print ('Generated list of sentences..')\n",
    "\n",
    "MAX_SEQ_LENGTH = 69\n",
    "MAX_NB_WORDS = 40000\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('./glove.6B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5250 unique tokens.\n",
      "Shape of data tensor: (3044, 69)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS, lower=False)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH, padding='post')\n",
    "print('Shape of data tensor:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3044, 69)\n",
      "(3044, 69)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:38: FutureWarning: The behavior of this method will change in future versions.  Use specific 'len(elem)' or 'elem is not None' test instead.\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# from keras.preprocessing.text import text_to_word_sequence\n",
    "# raw_output = corpus.findall('.//sentence')\n",
    "# train_out= np.zeros(shape=(3044,69))\n",
    "# i=0\n",
    "# for output in raw_output:\n",
    "#     s = text_to_word_sequence(output.find('text').text, lower=False)\n",
    "#     indices = np.zeros(MAX_SEQ_LENGTH)\n",
    "    \n",
    "#     aspectTerms = output.find('aspectTerms')\n",
    "#     if (aspectTerms):\n",
    "#         aspectTerm = aspectTerms.findall('aspectTerm')\n",
    "#         if (aspectTerm):\n",
    "#             for aspect_term in aspectTerm:\n",
    "#                 try:\n",
    "#                     indices[s.index(aspect_term.attrib['term'])] = 1\n",
    "# #                     print (indices)\n",
    "#                 except:\n",
    "#                     continue\n",
    "#     train_out[i] = indices\n",
    "#     i=i+1\n",
    "\n",
    "# print (\"Shape of output tensor:\", train_out.shape)\n",
    "import nltk\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "raw_output = corpus.findall('.//sentence')\n",
    "train_out = []\n",
    "delet = []\n",
    "print(data.shape)\n",
    "data = np.array(data)\n",
    "print(data.shape)\n",
    "i=0\n",
    "for output in raw_output:\n",
    "    s = text_to_word_sequence(output.find('text').text, lower=True)\n",
    "    indices = np.zeros(MAX_SEQ_LENGTH)\n",
    "    \n",
    "    aspectTerms = output.find('aspectTerms')\n",
    "    if (aspectTerms):\n",
    "        aspectTerm = aspectTerms.findall('aspectTerm')\n",
    "        k=0\n",
    "        if (len(aspectTerm)>0):\n",
    "            for aspect_term in aspectTerm:\n",
    "                try:\n",
    "                    aspt = text_to_word_sequence(aspect_term.attrib['term'])\n",
    "                    if(len(aspt) < 2):\n",
    "                        indices[s.index(aspt[0])] = 1\n",
    "                    else:\n",
    "                        k=1\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "    else:\n",
    "        k=1\n",
    "    if(k==1):\n",
    "          delet.append(i)\n",
    "    train_out.append(indices)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "Embedding Layer set..\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "nb_words = len(word_index)\n",
    "embedding_matrix = np.zeros((nb_words + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# Here, we have set trainable = False so as to keep the embeddings fixed.\n",
    "embedding_layer = Embedding(nb_words + 1,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQ_LENGTH,\n",
    "                            trainable=False)\n",
    "print('Embedding Layer set..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated word Embeddings..\n",
      "Shape of Embedding_output (3044, 69, 300)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "embedding_model = Sequential()\n",
    "embedding_model.add(embedding_layer)\n",
    "\n",
    "embedding_model.compile(loss='categorical_crossentropy',\n",
    "                        optimizer='rmsprop',\n",
    "                        metrics=['acc']\n",
    "                       )\n",
    "embedding_output = embedding_model.predict(data)\n",
    "print('Generated word Embeddings..')\n",
    "print('Shape of Embedding_output', embedding_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.text import text_to_word_sequence\n",
    "# from nltk.tag.stanford import StanfordPOSTagger\n",
    "# from sklearn import preprocessing\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# train_input = np.zeros(shape=(3044,69,306))\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# tags = [\"CC\",\"NN\",\"JJ\",\"VB\",\"RB\",\"IN\"]\n",
    "# le.fit(tags)\n",
    "# i=0\n",
    "# sentences = corpus.findall('.//sentence')\n",
    "# for sent in sentences:\n",
    "#     s = text_to_word_sequence(sent.find('text').text)\n",
    "#     tags_for_sent = nltk.pos_tag(s)\n",
    "#     sent_len = len(tags_for_sent)\n",
    "#     ohe = [0]*6\n",
    "\n",
    "#     for j in xrange(69):\n",
    "#         if j< len(tags_for_sent) and tags_for_sent[j][1][:2] in tags:\n",
    "#             ohe[le.transform(tags_for_sent[j][1][:2])] = 1\n",
    "#         train_input[i][j] = np.concatenate([embedding_output[i][j],ohe])\n",
    "#     i=i+1\n",
    "    \n",
    "# print('Concatenated Word-Embeddings and POS Tag Features..')\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_input = np.zeros(shape=(3044,69,306))\n",
    "le = preprocessing.LabelEncoder()\n",
    "tags = [\"CC\",\"NN\",\"JJ\",\"VB\",\"RB\",\"IN\"]\n",
    "le.fit(tags)\n",
    "i=0\n",
    "sentences = corpus.findall('.//sentence')\n",
    "for sent in sentences:\n",
    "    s = text_to_word_sequence(sent.find('text').text)\n",
    "    tags_for_sent = nltk.pos_tag(s)\n",
    "    sent_len = len(tags_for_sent)\n",
    "    ohe = [0]*6\n",
    "        \n",
    "    for j in range(69):\n",
    "        if j< len(tags_for_sent) and tags_for_sent[j][1][:2] in tags:\n",
    "            ohe[le.transform(tags_for_sent[j][1][:2])] = 1\n",
    "        train_input[i][j] = np.concatenate([embedding_output[i][j],ohe])\n",
    "    i=i+1\n",
    "    \n",
    "for i in sorted(delet, reverse=True):\n",
    "    train_input = np.delete(train_input, (i), axis=0)\n",
    "    train_out = np.delete(train_out, (i), axis=0)\n",
    "print('Concatenated Word-Embeddings and POS Tag Features...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution1D, Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling1D, MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import *\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model...\n",
      "Model Trained.\n"
     ]
    }
   ],
   "source": [
    "print('Training Model...')\n",
    "model = Sequential()\n",
    "model.add(Convolution1D(100, 5, border_mode=\"same\", input_shape=(69, 306)))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(MaxPooling1D(pool_length=5))\n",
    "model.add(Convolution1D(50, 3, border_mode=\"same\"))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(Activation(\"tanh\"))\n",
    "# softmax classifier\n",
    "model.add(Dense(69, W_regularizer=l2(0.01)))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "# model.load_weights('aspect_model_wepos.h5')\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print('Model Trained.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2739 samples, validate on 305 samples\n",
      "Epoch 1/50\n",
      "2739/2739 [==============================] - 7s - loss: 2.9180 - acc: 0.2172 - val_loss: 2.2590 - val_acc: 0.4066\n",
      "Epoch 2/50\n",
      "2739/2739 [==============================] - 6s - loss: 2.1503 - acc: 0.3866 - val_loss: 2.0058 - val_acc: 0.5213\n",
      "Epoch 3/50\n",
      "2739/2739 [==============================] - 6s - loss: 1.7007 - acc: 0.4655 - val_loss: 1.9009 - val_acc: 0.4033\n",
      "Epoch 4/50\n",
      "2739/2739 [==============================] - 5s - loss: 1.4165 - acc: 0.5016 - val_loss: 1.8314 - val_acc: 0.3934\n",
      "Epoch 5/50\n",
      "2739/2739 [==============================] - 6s - loss: 1.2879 - acc: 0.4987 - val_loss: 1.7569 - val_acc: 0.4459\n",
      "Epoch 6/50\n",
      "2739/2739 [==============================] - 6s - loss: 1.1982 - acc: 0.4951 - val_loss: 1.7188 - val_acc: 0.4328\n",
      "Epoch 7/50\n",
      "2739/2739 [==============================] - 6s - loss: 1.1385 - acc: 0.5177 - val_loss: 1.6686 - val_acc: 0.3902\n",
      "Epoch 8/50\n",
      "2739/2739 [==============================] - 6s - loss: 1.0936 - acc: 0.5035 - val_loss: 1.6892 - val_acc: 0.4262\n",
      "Epoch 9/50\n",
      "2739/2739 [==============================] - 5s - loss: 1.0515 - acc: 0.5024 - val_loss: 1.6391 - val_acc: 0.4328\n",
      "Epoch 10/50\n",
      "2739/2739 [==============================] - 5s - loss: 1.0168 - acc: 0.5097 - val_loss: 1.6346 - val_acc: 0.4656\n",
      "Epoch 11/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.9960 - acc: 0.5188 - val_loss: 1.6798 - val_acc: 0.4689\n",
      "Epoch 12/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.9677 - acc: 0.5108 - val_loss: 1.6133 - val_acc: 0.4230\n",
      "Epoch 13/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.9503 - acc: 0.5064 - val_loss: 1.6620 - val_acc: 0.4689\n",
      "Epoch 14/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.9383 - acc: 0.5078 - val_loss: 1.6064 - val_acc: 0.4131\n",
      "Epoch 15/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.9232 - acc: 0.5250 - val_loss: 1.6026 - val_acc: 0.4689\n",
      "Epoch 16/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.9103 - acc: 0.5082 - val_loss: 1.5695 - val_acc: 0.4525\n",
      "Epoch 17/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8981 - acc: 0.5093 - val_loss: 1.5730 - val_acc: 0.4754\n",
      "Epoch 18/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8769 - acc: 0.5013 - val_loss: 1.6171 - val_acc: 0.4459\n",
      "Epoch 19/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8844 - acc: 0.5104 - val_loss: 1.6029 - val_acc: 0.4459\n",
      "Epoch 20/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8642 - acc: 0.5100 - val_loss: 1.5999 - val_acc: 0.4689\n",
      "Epoch 21/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8624 - acc: 0.5232 - val_loss: 1.6554 - val_acc: 0.4689\n",
      "Epoch 22/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8558 - acc: 0.5089 - val_loss: 1.5912 - val_acc: 0.4590\n",
      "Epoch 23/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8592 - acc: 0.4991 - val_loss: 1.6156 - val_acc: 0.4590\n",
      "Epoch 24/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8349 - acc: 0.5130 - val_loss: 1.6224 - val_acc: 0.5016\n",
      "Epoch 25/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8464 - acc: 0.5104 - val_loss: 1.5855 - val_acc: 0.4689\n",
      "Epoch 26/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8264 - acc: 0.5060 - val_loss: 1.6052 - val_acc: 0.4623\n",
      "Epoch 27/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8264 - acc: 0.5119 - val_loss: 1.6301 - val_acc: 0.4689\n",
      "Epoch 28/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8214 - acc: 0.5115 - val_loss: 1.6320 - val_acc: 0.4754\n",
      "Epoch 29/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8179 - acc: 0.5115 - val_loss: 1.6719 - val_acc: 0.4820\n",
      "Epoch 30/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8118 - acc: 0.5122 - val_loss: 1.6300 - val_acc: 0.4754\n",
      "Epoch 31/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8136 - acc: 0.5071 - val_loss: 1.7103 - val_acc: 0.4689\n",
      "Epoch 32/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8163 - acc: 0.5192 - val_loss: 1.6055 - val_acc: 0.5049\n",
      "Epoch 33/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8049 - acc: 0.5064 - val_loss: 1.7017 - val_acc: 0.4656\n",
      "Epoch 34/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8078 - acc: 0.5162 - val_loss: 1.6554 - val_acc: 0.5082\n",
      "Epoch 35/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8066 - acc: 0.5184 - val_loss: 1.6104 - val_acc: 0.4984\n",
      "Epoch 36/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8011 - acc: 0.5035 - val_loss: 1.6439 - val_acc: 0.4721\n",
      "Epoch 37/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.7960 - acc: 0.5159 - val_loss: 1.6722 - val_acc: 0.4492\n",
      "Epoch 38/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.7978 - acc: 0.5217 - val_loss: 1.6356 - val_acc: 0.4492\n",
      "Epoch 39/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.7879 - acc: 0.5173 - val_loss: 1.6731 - val_acc: 0.4590\n",
      "Epoch 40/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7997 - acc: 0.5290 - val_loss: 1.6757 - val_acc: 0.4492\n",
      "Epoch 41/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7845 - acc: 0.5075 - val_loss: 1.6514 - val_acc: 0.4787\n",
      "Epoch 42/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.7998 - acc: 0.5078 - val_loss: 1.6361 - val_acc: 0.4852\n",
      "Epoch 43/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.7929 - acc: 0.5199 - val_loss: 1.6204 - val_acc: 0.5016\n",
      "Epoch 44/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7840 - acc: 0.5075 - val_loss: 1.6612 - val_acc: 0.4754\n",
      "Epoch 45/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7901 - acc: 0.5254 - val_loss: 1.5997 - val_acc: 0.4656\n",
      "Epoch 46/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7851 - acc: 0.5298 - val_loss: 1.6328 - val_acc: 0.4787\n",
      "Epoch 47/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7899 - acc: 0.5184 - val_loss: 1.6047 - val_acc: 0.4820\n",
      "Epoch 48/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7841 - acc: 0.5232 - val_loss: 1.6338 - val_acc: 0.5213\n",
      "Epoch 49/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7843 - acc: 0.5130 - val_loss: 1.6265 - val_acc: 0.4656\n",
      "Epoch 50/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7817 - acc: 0.5195 - val_loss: 1.5812 - val_acc: 0.4918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff26dcd4610>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_input, train_out,\n",
    "          validation_split=0.1,\n",
    "          batch_size=10,\n",
    "          nb_epoch=50\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('aspect_wepos.h5')\n",
    "y_pred = model.predict(train_input[2739:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_output = []\n",
    "for i in xrange(y_pred.shape[0]):\n",
    "    processed_label =[]\n",
    "    for j in xrange(y_pred.shape[1]):\n",
    "        if y_pred[i][j] > 0.42:\n",
    "            processed_label.append(1)\n",
    "        else:\n",
    "            processed_label.append(0)\n",
    "    processed_output.append(processed_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lookup(tokenizer, vec, returnIntNotWord=True):\n",
    "    twordkey = [(k, tokenizer.word_index[k]) for k in sorted(tokenizer.word_index, key=tokenizer.word_index.get, reverse=False)]\n",
    "    oneHotVec = [] #captures the index of the ords\n",
    "    engVec = [] #this one returns the indexs and the words. Make sure returnIntNotWord is false though\n",
    "    for eachRow, notUsed in enumerate(vec):\n",
    "        for index, item in enumerate(vec[0]):\n",
    "            if vec[eachRow][index] == 1:\n",
    "                oneHotVec.append(index)\n",
    "    for index in oneHotVec:\n",
    "        engVec.append(twordkey[index])\n",
    "    if returnIntNotWord == True:\n",
    "        return oneHotVec\n",
    "    else:\n",
    "        return engVec\n",
    "test_data = train_out[2739:]\n",
    "total_pos = 0.0\n",
    "true_pos = 0.0\n",
    "total_neg = 0.0\n",
    "true_neg = 0.0\n",
    "for i in xrange(test_data.shape[0]):\n",
    "    for j in xrange(test_data.shape[1]):\n",
    "        if test_data[i][j] == 1:\n",
    "            total_pos += 1\n",
    "            if processed_output[i][j] ==1:\n",
    "                true_pos +=1\n",
    "            if processed_output[i][j] == 0:\n",
    "                print(lookup(tokenizer,test_data[i],True)\n",
    "        if test_data[i][j] == 0:\n",
    "            total_neg += 1\n",
    "            if processed_output[i][j] ==0:\n",
    "                true_neg += 1\n",
    "\n",
    "false_pos = total_neg-true_neg\n",
    "false_neg = total_pos-true_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = true_pos/(true_pos+false_pos)\n",
    "recall = true_pos/total_pos\n",
    "f1_score = 2*precision*recall/(precision+recall)\n",
    "print (\"precision - \" +str(precision) + \", recall- \" +str(recall)+ \", f1_score- \" +str(f1_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:condatest] *",
   "language": "python",
   "name": "conda-env-condatest-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
